{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, cross_val_predict, train_test_split, KFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, roc_curve, balanced_accuracy_score, accuracy_score, mean_squared_error\n",
    "from scipy.stats import pearsonr, ttest_rel\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE, Isomap\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "# import xgboost as xgb\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "# import loralib as lora\n",
    "import random\n",
    "# import umap\n",
    "import re\n",
    "from itertools import product\n",
    "import os\n",
    "\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "    \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.head_dim = d_model // nhead\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.attn_weights = None\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        Q = self.q_linear(query).view(batch_size, -1, self.nhead, self.head_dim).transpose(1, 2)\n",
    "        K = self.k_linear(key).view(batch_size, -1, self.nhead, self.head_dim).transpose(1, 2)\n",
    "        V = self.v_linear(value).view(batch_size, -1, self.nhead, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn = self.dropout(torch.softmax(scores, dim=-1))\n",
    "\n",
    "        self.attn_weights = attn.detach()\n",
    "        \n",
    "        context = torch.matmul(attn, V)\n",
    "\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.out_linear(context)\n",
    "\n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, dim_feedforward, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(torch.relu(self.linear1(x)))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, nhead, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, dim_feedforward, dropout)\n",
    "        self.norm1 = nn.Identity()\n",
    "        self.norm2 = nn.Identity()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        src2 = self.self_attn(src, src, src, mask=src_mask)\n",
    "        src = src + self.dropout(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.feed_forward(src)\n",
    "        src = src + self.dropout(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout, num_layers):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        # Create new encoder layer instances directly instead of copying one\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
    "        output = src\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        return output\n",
    "    \n",
    "class TransformerClassificationNet(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward, output_dim, dropout=0.1, num_layers_frozen=0):\n",
    "        super(TransformerClassificationNet, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.num_layers_frozen = num_layers_frozen\n",
    "        self.transformer_encoder = TransformerEncoder(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            num_layers=num_layers\n",
    "        )        \n",
    "        self.decoder = nn.Linear(d_model, output_dim)\n",
    "        self.d_model = d_model\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self._init_transformer_weights()\n",
    "\n",
    "    def _init_transformer_weights(self):\n",
    "        def _init_layer_weights(layer):\n",
    "            for name, param in layer.named_parameters():\n",
    "                if param.dim() > 1:\n",
    "                    nn.init.xavier_uniform_(param.data)\n",
    "                else:\n",
    "                    nn.init.uniform_(param.data, -0.1, 0.1)\n",
    "\n",
    "        for layer in self.transformer_encoder.layers:\n",
    "            _init_layer_weights(layer)\n",
    "    def forward(self, src, src_key_padding_mask=None, return_latent=False, return_attentions=False):\n",
    "        def print_sample_info(tensor, name, num_samples=5, num_values=5):\n",
    "            print(f\"\\n{name} shape: {tensor.shape}\")\n",
    "            for i in range(min(num_samples, tensor.shape[0])):\n",
    "                if tensor.dim() > 1:\n",
    "                    sample_values = tensor[i, :num_values].tolist()\n",
    "                    if tensor.dtype in [torch.float32, torch.float64]:\n",
    "                        sample_mean = tensor[i].mean().item()\n",
    "                        sample_std = tensor[i].std().item()\n",
    "                        print(f\"Sample {i}: First {num_values} values: {sample_values}\")\n",
    "                        print(f\"Sample {i} mean: {sample_mean:.4f}, std: {sample_std:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"Sample {i}: First {num_values} values: {sample_values}\")\n",
    "                else:\n",
    "                    sample_values = tensor[i].tolist()\n",
    "                    print(f\"Sample {i}: {sample_values}\")\n",
    "\n",
    "        #print_sample_info(src, \"Input\")\n",
    "\n",
    "        embedded = self.embedding(src) * math.sqrt(self.embedding.embedding_dim)\n",
    "        #print_sample_info(embedded, \"Embedded\")\n",
    "\n",
    "        pos_encoded = self.pos_encoder(embedded)\n",
    "        #print_sample_info(pos_encoded, \"Pos encoded\")\n",
    "\n",
    "        output = self.transformer_encoder(pos_encoded, src_key_padding_mask=src_key_padding_mask)\n",
    "        #print_sample_info(output, \"Transformer output\")\n",
    "        #output = self.decoder(output[:, -1, :])\n",
    "\n",
    "        if return_attentions:\n",
    "            attn_weights = []\n",
    "            for layer in self.transformer_encoder.layers:\n",
    "                if hasattr(layer.self_attn, 'attn_weights'):\n",
    "                    attn_weights.append(layer.self_attn.attn_weights)\n",
    "        \n",
    "        #print_sample_info(output, \"Mean pooled\")\n",
    "        output = output.mean(dim=1)\n",
    "        latent = output.clone()\n",
    "        output = self.decoder(output)\n",
    "        #print_sample_info(output, \"Decoder output\")\n",
    "\n",
    "        final_output = torch.sigmoid(output)\n",
    "        #print_sample_info(final_output, \"Final output\")\n",
    "        if return_latent and return_attentions:\n",
    "            return final_output, latent, attn_weights\n",
    "        elif return_latent:\n",
    "            return final_output, latent\n",
    "        elif return_attentions:\n",
    "            return final_output, attn_weights\n",
    "        else:\n",
    "            return final_output\n",
    "\n",
    "    def load_model(self, path):\n",
    "        state_dict = torch.load(path)\n",
    "        model_dict = self.state_dict()\n",
    "        \n",
    "        # print(\"Keys in state_dict (pretrained model):\")\n",
    "        # print(sorted(state_dict.keys()))\n",
    "        # print(\"\\nKeys in model_dict (current model):\")\n",
    "        # print(sorted(model_dict.keys()))\n",
    "        \n",
    "        # Filter out unnecessary keys and handle mismatches\n",
    "        new_state_dict = {}\n",
    "        for key, value in state_dict.items():\n",
    "            if key in model_dict:\n",
    "                if model_dict[key].shape == value.shape:\n",
    "                    new_state_dict[key] = value                \n",
    "        \n",
    "        # Print out any missing or unexpected keys\n",
    "        missing_keys = set(model_dict.keys()) - set(new_state_dict.keys())\n",
    "        unexpected_keys = set(state_dict.keys()) - set(model_dict.keys())\n",
    "        \n",
    "        if missing_keys:\n",
    "            print(f\"Missing keys in loaded model: {missing_keys}\")\n",
    "        if unexpected_keys:\n",
    "            print(f\"Unexpected keys in loaded model: {unexpected_keys}\")\n",
    "        \n",
    "        # Load the filtered state dict\n",
    "        self.load_state_dict(new_state_dict, strict=False)\n",
    "            \n",
    "    def freeze_transformer(self):\n",
    "        \"\"\"\n",
    "        Freezes the first num_layers_frozen layers of the transformer,\n",
    "        along with embeddings and positional encodings if any layers are frozen.\n",
    "        Assumes 4-layer transformer.\n",
    "        \"\"\"\n",
    "        if self.num_layers_frozen > 0:\n",
    "            # If we're freezing any layers, also freeze embeddings and positional encodings\n",
    "            for param in self.embedding.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.pos_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            # Freeze the specified number of transformer layers from the bottom\n",
    "            for i, layer in enumerate(self.transformer_encoder.layers):\n",
    "                if i < self.num_layers_frozen:  # Freeze early layers\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "                else:  # Keep later layers trainable\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = True\n",
    "                        \n",
    "        # Print freezing status for verification\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"Trainable: {name}\")\n",
    "            else:\n",
    "                print(f\"Frozen: {name}\")\n",
    "\n",
    "\n",
    "class PatientDataset(Dataset):\n",
    "    def __init__(self, data, labels, lengths):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.lengths = lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx], self.lengths[idx]\n",
    "\n",
    "class EHRDataset(Dataset):\n",
    "    def __init__(self, codes, outcomes, lengths):\n",
    "        self.codes = codes\n",
    "        self.outcomes = torch.tensor(outcomes, dtype=torch.float32)  # Convert to tensor here\n",
    "        self.lengths = lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.outcomes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.codes[idx], self.outcomes[idx], self.lengths[idx]\n",
    "\n",
    "def create_attention_mask(lengths, max_len):\n",
    "    device = lengths.device\n",
    "    mask = (torch.arange(max_len, device=device)[None, :] < lengths[:, None]).to(device)\n",
    "    return mask.transpose(0, 1)  # Transpose the mask to match the expected shape\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def collate_fn(batch):\n",
    "    if len(batch[0]) == 3:  # If proteomics data is included\n",
    "        data, labels, proteomics = zip(*batch)\n",
    "        data = pad_sequence([torch.LongTensor(d) for d in data], batch_first=True, padding_value=0)\n",
    "        return data, torch.tensor(labels, dtype=torch.float32), torch.tensor(proteomics, dtype=torch.float32)\n",
    "    else:\n",
    "        data, labels = zip(*batch)\n",
    "        data = pad_sequence([torch.LongTensor(d) for d in data], batch_first=True, padding_value=0)\n",
    "        return data, torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "def create_dataloader(EHR_codes, proteomics, outcomes, lengths, batch_size, feature_types):\n",
    "    dataset = EHRDataset(EHR_codes, outcomes, lengths)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def normalize_dataset(dataset):\n",
    "    for i, sequence in enumerate(dataset):\n",
    "        dataset[i] = (sequence - sequence.mean(dim=0, keepdim=True)) / (sequence.std(dim=0, keepdim=True) + 1e-8)\n",
    "    return dataset\n",
    "\n",
    "def impute_missing_values(dataset):\n",
    "    # Stack all tensors in the dataset along a new dimension, creating a tensor of shape (num_samples, max_seq_length, num_features)\n",
    "    stacked_data = torch.stack(dataset)\n",
    "\n",
    "    # Calculate the mean of each feature across all samples and sequences, ignoring NaN values\n",
    "    feature_means = torch.nanmean(stacked_data, dim=(0, 1))\n",
    "\n",
    "    # Iterate through the dataset (list of tensors)\n",
    "    for i, sequence in enumerate(dataset):\n",
    "        # Create a boolean mask indicating the positions of NaN values in the sequence\n",
    "        mask = torch.isnan(sequence)\n",
    "\n",
    "        # Replace NaN values in the sequence with the corresponding feature means\n",
    "        # 'expand_as' is used to match the dimensions of the mask and the sequence\n",
    "        dataset[i][mask] = feature_means.expand_as(sequence)[mask]\n",
    "\n",
    "    return dataset\n",
    "        \n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataBuilder(Dataset):\n",
    "    def __init__(self, x, y, standardizer):\n",
    "        self.x, self.y, self.standardizer = x, y, standardizer\n",
    "        self.len=self.x.shape[0]\n",
    "    def __getitem__(self,index):      \n",
    "        return (self.x[index], self.y[index])\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for this project, feature_types is always EHR\n",
    "def run_experiment(EHR_tokens, proteomics, patient_indices, outcomes, lengths, experiment_name, lr, lr_decay,\n",
    "                   bs, prot_hidden_dim=32, train_indices=None, val_indices=None, test_indices=None, feature_types='EHR', model_path='', fine_tune=False, seed=42, num_layers=2, hidden_dim=400,\n",
    "                   dropout=0.4, return_preds=False, return_interpretability=False, return_grads=False,\n",
    "                   protein_weights=None, blend=None, hyperparam_tuning=False, vocab_size=None, num_frozen=0):\n",
    "    \n",
    "    set_seed(seed)\n",
    "    prediction_module_hidden_sizes = [hidden_dim, hidden_dim//2, hidden_dim//4, hidden_dim//8]\n",
    "    \n",
    "    assert feature_types in ['EHR', 'metab', 'both']   \n",
    "    if feature_types == 'metab': assert model_path == ''\n",
    "    if (model_path != '') & (feature_types == 'both'): assert fine_tune==True\n",
    "    if hyperparam_tuning == False: assert train_indices == None\n",
    "\n",
    "    if hyperparam_tuning == False:\n",
    "        maternal_IDs = patient_indices['person_id']\n",
    "        train_ratio, test_ratio, val_ratio = 0.70, 0.15, 0.15\n",
    "        train_ids, temp_ids = train_test_split(maternal_IDs, test_size=(test_ratio + val_ratio), random_state=seed, stratify=outcomes)\n",
    "        patient_indices_temp = patient_indices.copy(deep=True)\n",
    "        patient_indices_temp.index = patient_indices_temp['person_id']\n",
    "        temp_indices = patient_indices_temp.loc[temp_ids, 'index'].values\n",
    "        test_ids, val_ids = train_test_split(temp_ids, test_size=(val_ratio / (test_ratio + val_ratio)), random_state=seed, stratify=outcomes[temp_indices])\n",
    "        \n",
    "        if feature_types != 'EHR':\n",
    "            proteomics = proteomics.merge(patient_indices[['person_id','index']], how='left', left_on='person_id', right_on='person_id').drop(['person_id'],axis=1).sort_values('index').drop('index',axis=1).values\n",
    "\n",
    "        train_indices = patient_indices[patient_indices['person_id'].isin(train_ids)]['index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices[patient_indices['person_id'].isin(test_ids)]['index'].values\n",
    "        val_indices = patient_indices[patient_indices['person_id'].isin(val_ids)]['index'].values\n",
    "\n",
    "    # Prepare data\n",
    "    train_EHR_tokens = EHR_tokens[train_indices, :]\n",
    "    test_EHR_tokens = EHR_tokens[test_indices, :]\n",
    "    val_EHR_tokens = EHR_tokens[val_indices, :]\n",
    "    \n",
    "    if feature_types != 'EHR':\n",
    "        train_proteomics = proteomics[train_indices, :]\n",
    "        test_proteomics = proteomics[test_indices, :]\n",
    "        val_proteomics = proteomics[val_indices, :]\n",
    "        scaler = StandardScaler()\n",
    "        train_proteomics = scaler.fit_transform(train_proteomics)\n",
    "        test_proteomics = scaler.transform(test_proteomics)\n",
    "        val_proteomics = scaler.transform(val_proteomics)\n",
    "    else:\n",
    "        train_proteomics = test_proteomics = val_proteomics = None\n",
    "\n",
    "    train_outcomes = outcomes[train_indices]\n",
    "    test_outcomes = outcomes[test_indices]\n",
    "    val_outcomes = outcomes[val_indices]\n",
    "\n",
    "    train_lengths = lengths[train_indices]\n",
    "    test_lengths = lengths[test_indices]\n",
    "    val_lengths = lengths[val_indices]\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = create_dataloader(train_EHR_tokens, train_proteomics, train_outcomes, train_lengths, bs, feature_types)\n",
    "    test_loader = create_dataloader(test_EHR_tokens, test_proteomics, test_outcomes, test_lengths, bs, feature_types)\n",
    "    val_loader = create_dataloader(val_EHR_tokens, val_proteomics, val_outcomes, val_lengths, bs, feature_types)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    # Model initialization\n",
    "    if vocab_size == None:\n",
    "        vocab_size = EHR_tokens.max().item() + 1\n",
    "    if model_path == '':\n",
    "        if feature_types == 'EHR':\n",
    "            print(f\"Initializing TransformerClassificationNet with parameters:\")\n",
    "            print(f\"vocab_size: {vocab_size}, d_model: {hidden_dim}, nhead: 4, num_layers: {num_layers}, dim_feedforward: {hidden_dim*4}, output_size: 1, dropout: {dropout}\")\n",
    "            try:\n",
    "                model = TransformerClassificationNet(\n",
    "                    vocab_size=vocab_size, \n",
    "                    d_model=hidden_dim, \n",
    "                    nhead=4,\n",
    "                    num_layers=num_layers, \n",
    "                    dim_feedforward=hidden_dim * 4,\n",
    "                    output_dim=1,\n",
    "                    dropout=dropout,\n",
    "                    num_layers_frozen=num_frozen\n",
    "                )\n",
    "                model.to(device)\n",
    "                if num_frozen > 0:\n",
    "                    model.freeze_transformer()\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error initializing model: {str(e)}\")\n",
    "                print(\"CUDA error: device-side assert triggered. This might be due to incompatible dimensions or insufficient GPU memory.\")\n",
    "                print(f\"EHR_tokens shape: {EHR_tokens.shape}\")\n",
    "                print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "                print(f\"Current GPU memory usage: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "                raise\n",
    "        elif feature_types == 'metab':\n",
    "            model = proteomics_net(proteomics.shape[1], [prot_hidden_dim], 1, dropout).to(device)\n",
    "        elif feature_types == 'both':\n",
    "            model = JointTransformerClassification(\n",
    "                vocab_size=vocab_size,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=num_layers,\n",
    "                output_size=1,\n",
    "                input_size_proteomics=proteomics.shape[1],\n",
    "                proteomics_hidden_layers=[prot_hidden_dim],\n",
    "                nhead=4,\n",
    "                dropout=dropout\n",
    "            ).to(device)\n",
    "    else:\n",
    "        if feature_types == 'EHR':\n",
    "            model = TransformerClassificationNet(\n",
    "                vocab_size=vocab_size, \n",
    "                d_model=hidden_dim, \n",
    "                nhead=4,\n",
    "                num_layers=num_layers, \n",
    "                dim_feedforward=hidden_dim * 4,\n",
    "                output_dim=1,\n",
    "                dropout=dropout,\n",
    "                num_layers_frozen=num_frozen\n",
    "            )\n",
    "            model.load_model(model_path)\n",
    "            model.to(device)\n",
    "            if num_frozen > 0:\n",
    "                model.freeze_transformer()\n",
    "            if not fine_tune:\n",
    "                model.eval()\n",
    "                return evaluate_model(model, val_loader, device, return_preds)\n",
    "        elif feature_types == 'both':\n",
    "            model = JointTransformerClassification(\n",
    "                vocab_size=vocab_size,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=num_layers,\n",
    "                output_size=1,\n",
    "                input_size_proteomics=proteomics.shape[1],\n",
    "                proteomics_hidden_layers=[prot_hidden_dim],\n",
    "                nhead=4,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            model.load_model(model_path)\n",
    "            model.to(device)\n",
    "            if fine_tune:\n",
    "                model.freeze_transformer()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=lr_decay)\n",
    "    num_epochs = 100\n",
    "    train_losses, test_losses, val_losses = [], [], []\n",
    "    best_loss = np.inf\n",
    "    patience, counter = 5, 0\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        if not hyperparam_tuning:\n",
    "            torch.save(model.state_dict(), f'./models/predictive_models/{experiment_name}_epoch{epoch}.pth')\n",
    "        \n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device, feature_types, blend, PT_EHR_model if blend else None)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss, predictions, true_labels = evaluate_epoch(model, test_loader, criterion, device, feature_types)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        pearson_corr = roc_auc_score(true_labels, predictions)\n",
    "        print(f'Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test AUROC: {pearson_corr:.4f}')\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), f'./models/predictive_models/{experiment_name}.pth')\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load(f'./models/predictive_models/{experiment_name}.pth'))\n",
    "    val_loss, val_predictions, val_true_labels = evaluate_epoch(model, val_loader, criterion, device, feature_types)\n",
    "    val_losses.append(val_loss)\n",
    "    pearson_corr = roc_auc_score(val_true_labels, val_predictions)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, AUC: {pearson_corr:.4f}')\n",
    "\n",
    "    if hyperparam_tuning:\n",
    "        os.remove(f'./models/predictive_models/{experiment_name}.pth')\n",
    "\n",
    "    if return_preds:\n",
    "        return pearson_corr, val_loss, None, val_outcomes, np.array(val_predictions), val_indices\n",
    "    else:\n",
    "        return pearson_corr, val_loss, None\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, device, feature_types, blend, PT_EHR_model):\n",
    "    running_loss, num_samples = 0, 0\n",
    "    for batch_idx, batch in enumerate(loader):\n",
    "        if feature_types == 'EHR':\n",
    "            tokens, labels, lengths = batch\n",
    "            tokens, labels, lengths = tokens.to(device), labels.to(device), lengths.to(device)\n",
    "            proteomics = None\n",
    "        elif feature_types == 'metab':\n",
    "            proteomics, labels = batch\n",
    "            proteomics, labels = proteomics.to(device), labels.to(device)\n",
    "        else:\n",
    "            tokens, proteomics, labels, lengths = batch\n",
    "            tokens, proteomics, labels, lengths = tokens.to(device), proteomics.to(device), labels.to(device), lengths.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if blend:\n",
    "            with torch.no_grad():\n",
    "                _, (optimal_latent, _, _, _, _, _) = PT_EHR_model(tokens, None, lengths, interpretability=True)\n",
    "            outputs = model(tokens, proteomics, lengths, better_latent=optimal_latent, better_ratio=0.5)\n",
    "        else:\n",
    "            if feature_types == 'EHR':\n",
    "                outputs = model(tokens, lengths)\n",
    "            elif feature_types == 'metab':\n",
    "                proteomics = proteomics.float()\n",
    "                outputs = model(proteomics)\n",
    "            else:  # 'both'\n",
    "                proteomics = proteomics.float()\n",
    "                outputs = model(tokens, proteomics, lengths)\n",
    "\n",
    "        loss = criterion(outputs.squeeze(-1), labels)  # Only squeeze the last dimension\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        num_samples += labels.size(0)\n",
    "\n",
    "    return running_loss / num_samples\n",
    "\n",
    "def evaluate_epoch(model, loader, criterion, device, feature_types):\n",
    "    running_loss, num_samples = 0, 0\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if feature_types == 'EHR':\n",
    "                tokens, labels, lengths = batch\n",
    "                tokens, labels, lengths = tokens.to(device), labels.to(device), lengths.to(device)\n",
    "                proteomics = None\n",
    "            elif feature_types == 'metab':\n",
    "                proteomics, labels = batch\n",
    "                proteomics, labels = proteomics.to(device), labels.to(device)\n",
    "            else:\n",
    "                tokens, proteomics, labels, lengths = batch\n",
    "                tokens, proteomics, labels, lengths = tokens.to(device), proteomics.to(device), labels.to(device), lengths.to(device)\n",
    "\n",
    "            if feature_types == 'EHR':\n",
    "                outputs = model(tokens, lengths)\n",
    "            elif feature_types == 'metab':\n",
    "                proteomics = proteomics.float()\n",
    "                outputs = model(proteomics)\n",
    "            else:  # 'both'\n",
    "                proteomics = proteomics.float()\n",
    "                outputs = model(tokens, proteomics, lengths)\n",
    "\n",
    "            loss = criterion(outputs.squeeze(-1), labels)  # Only squeeze the last dimension\n",
    "\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            num_samples += labels.size(0)\n",
    "            predictions.extend(outputs.squeeze(-1).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return running_loss / num_samples, predictions, true_labels\n",
    "\n",
    "def evaluate_model(model, loader, device, return_preds):\n",
    "    criterion = nn.BCELoss()\n",
    "    val_loss, val_predictions, val_true_labels = evaluate_epoch(model, loader, criterion, device, 'EHR')\n",
    "    pearson_corr = roc_auc_score(val_true_labels, val_predictions)\n",
    "    print(f'Validation Loss: {val_loss:.4f}, AUC: {pearson_corr:.4f}')\n",
    "    if return_preds:\n",
    "        return pearson_corr, val_loss, None, val_true_labels, val_predictions, None\n",
    "    else:\n",
    "        return pearson_corr, val_loss, None\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_best_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outcome_list = ['MACE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_TRIALS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_indices_PT = pd.read_csv('./processed_data/transformer_sample_id_to_index.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pre-training cohort data\n",
    "transformer_input_sequences_PT = np.load('./processed_data/transformer_input_sequences.npy')\n",
    "transformer_input_lengths_PT = np.load('./processed_data/transformer_input_lengths.npy')\n",
    "patient_indices_PT = pd.read_csv('./processed_data/transformer_sample_id_to_index.csv')\n",
    "transformer_outcomes_PT = np.load('./processed_data/transformer_mace_outcomes.npy')\n",
    "\n",
    "print(\"PT cohort data shapes:\")\n",
    "print(f\"Input sequences: {transformer_input_sequences_PT.shape}\")\n",
    "print(f\"Input lengths: {transformer_input_lengths_PT.shape}\")\n",
    "print(f\"Outcomes: {transformer_outcomes_PT.shape}\")\n",
    "print(f\"Patient indices: {patient_indices_PT.shape}\")\n",
    "\n",
    "# Load omics cohort data\n",
    "transformer_input_sequences_omics = np.load('./processed_data/transformer_input_sequences_FT.npy')\n",
    "transformer_input_lengths_omics = np.load('./processed_data/transformer_input_lengths_FT.npy')\n",
    "patient_indices_omics = pd.read_csv('./processed_data/transformer_sample_id_to_index_FT.csv')\n",
    "transformer_outcomes_omics = np.load('./processed_data/transformer_mace_outcomes_FT.npy')\n",
    "\n",
    "print(\"\\nOmics cohort data shapes:\")\n",
    "print(f\"Input sequences: {transformer_input_sequences_omics.shape}\")\n",
    "print(f\"Input lengths: {transformer_input_lengths_omics.shape}\")\n",
    "print(f\"Outcomes: {transformer_outcomes_omics.shape}\")\n",
    "print(f\"Patient indices: {patient_indices_omics.shape}\")\n",
    "\n",
    "# Load vocabularies\n",
    "with open('./processed_data/transformer_vocab.pkl', 'rb') as f:\n",
    "    vocab_PT = pickle.load(f)\n",
    "\n",
    "with open('./processed_data/transformer_vocab_FT.pkl', 'rb') as f:\n",
    "    vocab_omics = pickle.load(f)\n",
    "\n",
    "print(f\"\\nPT Vocabulary size: {len(vocab_PT)}\")\n",
    "print(f\"Omics Vocabulary size: {len(vocab_omics)}\")\n",
    "# Create a new merged vocabulary that preserves IDs from both original vocabularies\n",
    "merged_vocab = {}\n",
    "current_max_id = 0\n",
    "\n",
    "# First, add all tokens from the PT vocabulary\n",
    "for token, idx in vocab_PT.items():\n",
    "    merged_vocab[token] = idx\n",
    "    current_max_id = max(current_max_id, idx)\n",
    "\n",
    "# Then, add tokens from the omics vocabulary, preserving their IDs if not already present\n",
    "for token, idx in vocab_omics.items():\n",
    "    if token not in merged_vocab:\n",
    "        merged_vocab[token] = idx\n",
    "    current_max_id = max(current_max_id, idx)\n",
    "\n",
    "# Add any new tokens that might have different IDs in omics vocab\n",
    "for token, idx in vocab_omics.items():\n",
    "    if token not in merged_vocab:\n",
    "        current_max_id += 1\n",
    "        merged_vocab[token] = current_max_id\n",
    "\n",
    "print(f\"Merged Vocabulary size: {len(merged_vocab)}\")\n",
    "\n",
    "# Function to update token IDs based on merged vocabulary\n",
    "def update_token_ids(sequences, old_vocab, new_vocab):\n",
    "    old_vocab_reverse = {v: k for k, v in old_vocab.items()}\n",
    "    updated_sequences = np.zeros_like(sequences)\n",
    "    for i, seq in tqdm(enumerate(sequences), total=len(sequences)):\n",
    "        updated_seq = []\n",
    "        for token_id in seq:\n",
    "            if token_id in old_vocab_reverse:\n",
    "                token = old_vocab_reverse[token_id]\n",
    "                updated_seq.append(new_vocab[token])\n",
    "            else:\n",
    "                updated_seq.append(new_vocab['[PAD]'])\n",
    "        updated_sequences[i] = updated_seq\n",
    "    return updated_sequences\n",
    "\n",
    "# Update token IDs in both datasets\n",
    "transformer_input_sequences_PT = update_token_ids(transformer_input_sequences_PT, vocab_PT, merged_vocab)\n",
    "transformer_input_sequences_omics = update_token_ids(transformer_input_sequences_omics, vocab_omics, merged_vocab)\n",
    "\n",
    "print(\"\\nUpdated data shapes:\")\n",
    "print(f\"PT input sequences: {transformer_input_sequences_PT.shape}\")\n",
    "print(f\"Omics input sequences: {transformer_input_sequences_omics.shape}\")\n",
    "\n",
    "# Verification\n",
    "print(\"\\nVerification:\")\n",
    "print(f\"Number of unique tokens in PT sequences: {len(np.unique(transformer_input_sequences_PT))}\")\n",
    "print(f\"Number of unique tokens in Omics sequences: {len(np.unique(transformer_input_sequences_omics))}\")\n",
    "print(f\"Max token ID in PT sequences: {np.max(transformer_input_sequences_PT)}\")\n",
    "print(f\"Max token ID in Omics sequences: {np.max(transformer_input_sequences_omics)}\")\n",
    "\n",
    "# Check for ID consistency\n",
    "pt_sample = transformer_input_sequences_PT[0][:10]  # First 10 tokens of first sequence\n",
    "omics_sample = transformer_input_sequences_omics[0][:10]  # First 10 tokens of first sequence\n",
    "\n",
    "print(\"\\nSample token IDs:\")\n",
    "print(f\"PT: {pt_sample}\")\n",
    "print(f\"Omics: {omics_sample}\")\n",
    "merged_vocab_reverse = {v: k for k, v in merged_vocab.items()}\n",
    "\n",
    "# Decode samples using the merged vocabulary\n",
    "print(\"\\nDecoded PT sample with merged vocab:\")\n",
    "print([merged_vocab_reverse.get(token_id, '[UNK]') for token_id in pt_sample])\n",
    "\n",
    "print(\"\\nDecoded Omics sample with merged vocab:\")\n",
    "print([merged_vocab_reverse.get(token_id, '[UNK]') for token_id in omics_sample])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(transformer_outcomes_PT)/len(transformer_outcomes_PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(transformer_outcomes_omics)/len(transformer_outcomes_omics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "        print(f\"  Memory Cached:    {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"No CUDA-enabled GPU found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run cell below if loading old hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_best_params = pickle.load(open('best_hyperparams_transformer.pkl','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hyperparam Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing parameters: {'batch_size': 128, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 256}\n",
    "\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Hyperparameter grid\n",
    "grid_search = {\n",
    "    'batch_size': [128],\n",
    "    'lr': [1e-3, 1e-4],\n",
    "    'dropout': [0.1, 0.3, 0.5],\n",
    "    'lr_decay': [1e-4, 1e-3],\n",
    "    'layers': [1,2,4],\n",
    "    'hidden_dim': [128, 256]\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]\n",
    "\n",
    "# Create or load results file\n",
    "results_file = \"hyperparam_search_results.csv\"\n",
    "if os.path.exists(results_file):\n",
    "    results_df = pd.read_csv(results_file)\n",
    "    print(f\"Loaded {len(results_df)} existing results\")\n",
    "else:\n",
    "    results_df = pd.DataFrame(columns=[\n",
    "        'outcome', 'num_layers', 'dropout', 'lr', 'lr_decay', \n",
    "        'hidden_dim', 'batch_size', 'val_loss', 'val_auroc'  # Added val_auroc\n",
    "    ])\n",
    "\n",
    "# Main loop\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i] = {'PT': {}}\n",
    "    maternal_IDs = patient_indices_PT['person_id'].unique()\n",
    "    \n",
    "    # Subsample the data\n",
    "    subsample_size = int(0.01 * len(maternal_IDs))\n",
    "    subsampled_IDs = np.random.choice(maternal_IDs, subsample_size, replace=False, p=None)\n",
    "    \n",
    "    # Single train/val/test split\n",
    "    train_size = int(0.7 * len(subsampled_IDs))\n",
    "    val_size = int(0.15 * len(subsampled_IDs))\n",
    "    \n",
    "    # Split IDs\n",
    "    train_IDs = subsampled_IDs[:train_size]\n",
    "    val_IDs = subsampled_IDs[train_size:train_size + val_size]\n",
    "    test_IDs = subsampled_IDs[train_size + val_size:]\n",
    "    \n",
    "    # Get indices\n",
    "    train_indices = patient_indices_PT[patient_indices_PT['person_id'].isin(train_IDs)]['index'].values\n",
    "    test_indices = patient_indices_PT[patient_indices_PT['person_id'].isin(test_IDs)]['index'].values\n",
    "    val_indices = patient_indices_PT[patient_indices_PT['person_id'].isin(val_IDs)]['index'].values\n",
    "    np.random.shuffle(train_indices)\n",
    "    \n",
    "    print(f\"Dataset sizes after subsample:\")\n",
    "    print(f\"Total samples: {len(subsampled_IDs)}\")\n",
    "    print(f\"Train: {len(train_indices)}, Val: {len(val_indices)}, Test: {len(test_indices)}\")\n",
    "    \n",
    "    # Test all parameter combinations\n",
    "    for param_set in tqdm(all_params):\n",
    "        # Check if we've already tested these parameters for this outcome\n",
    "        param_match = (results_df['outcome'] == i)\n",
    "        for key, value in param_set.items():\n",
    "            if key == 'batch_size':\n",
    "                continue  # Skip batch_size as it's constant\n",
    "            # Map 'layers' to 'num_layers' for DataFrame comparison\n",
    "            df_key = 'num_layers' if key == 'layers' else key\n",
    "            param_match &= (results_df[df_key] == value)\n",
    "            \n",
    "        if param_match.any():\n",
    "            print(f\"Skipping parameter set - already tested\")\n",
    "            continue\n",
    "            \n",
    "        bs = param_set['batch_size']\n",
    "        lr = param_set['lr']\n",
    "        dropout = param_set['dropout']\n",
    "        lr_decay = param_set['lr_decay']\n",
    "        layers = param_set['layers']\n",
    "        hidden_dim = param_set['hidden_dim']\n",
    "        \n",
    "        model_name = f'PT_MODEL_{layers}_{lr}_{lr_decay}_{dropout}_{hidden_dim}'\n",
    "        print(f\"Testing parameters: {param_set}\")\n",
    "        \n",
    "        try:\n",
    "            val_auroc, val_loss, _ = run_experiment(  # Now correctly capturing the AUROC from first return value\n",
    "                transformer_input_sequences_PT, \n",
    "                None,\n",
    "                patient_indices_PT, \n",
    "                transformer_outcomes_PT, \n",
    "                transformer_input_lengths_PT, \n",
    "                model_name,\n",
    "                lr, \n",
    "                lr_decay, \n",
    "                bs,\n",
    "                train_indices=train_indices,\n",
    "                test_indices=test_indices,\n",
    "                val_indices=val_indices,\n",
    "                feature_types='EHR',\n",
    "                model_path='',\n",
    "                fine_tune=False,\n",
    "                seed=42,\n",
    "                hidden_dim=hidden_dim,\n",
    "                num_layers=layers,\n",
    "                dropout=dropout,\n",
    "                hyperparam_tuning=True,\n",
    "                vocab_size=len(merged_vocab)\n",
    "            )\n",
    "            \n",
    "            # Add result to DataFrame\n",
    "            new_result = pd.DataFrame({\n",
    "                'outcome': [i],\n",
    "                'num_layers': [layers],\n",
    "                'dropout': [dropout],\n",
    "                'lr': [lr],\n",
    "                'lr_decay': [lr_decay],\n",
    "                'hidden_dim': [hidden_dim],\n",
    "                'batch_size': [bs],\n",
    "                'val_loss': [val_loss],\n",
    "                'val_auroc': [val_auroc]  # Added val_auroc from first return value\n",
    "            })\n",
    "            \n",
    "            results_df = pd.concat([results_df, new_result], ignore_index=True)\n",
    "            \n",
    "            # Save after each experiment\n",
    "            results_df.to_csv(results_file, index=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with parameters {param_set}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f'PT\\noutcome {i}')\n",
    "\n",
    "    # Find best parameters for this outcome\n",
    "    outcome_results = results_df[results_df['outcome'] == i]\n",
    "    best_row = outcome_results.loc[outcome_results['val_loss'].idxmin()]\n",
    "    \n",
    "    model_name = f'PT_MODEL_{best_row[\"num_layers\"]}_{best_row[\"lr\"]}_{best_row[\"lr_decay\"]}_{best_row[\"dropout\"]}_{best_row[\"hidden_dim\"]}'\n",
    "    \n",
    "    overall_best_params[i]['PT'] = {\n",
    "        'num_layers': int(best_row['num_layers']),\n",
    "        'lr': best_row['lr'],\n",
    "        'lr_decay': best_row['lr_decay'],\n",
    "        'dropout': best_row['dropout'],\n",
    "        'hidden_dim': int(best_row['hidden_dim']),\n",
    "        'batch_size': int(best_row['batch_size']),\n",
    "        'model_name': model_name\n",
    "    }\n",
    "    \n",
    "    # Save best parameters after each outcome\n",
    "    with open(\"best_hyperparams_transformer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "# hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "# hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "# num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "# print(np.min(hyperparam_df['val_loss']))\n",
    "# model_name = 'PT_MODEL_{}_{}_{}_{}_{}'.format(num_layers,lr,lr_decay,dropout, hidden_dim)\n",
    "# overall_best_params['stroke']['PT'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "#                                 'hidden_dim': int(hidden_dim), 'batch_size': int(bs), 'model_name': model_name}\n",
    "# with open(\"best_hyperparams_transformer.pkl\", \"wb\") as f:\n",
    "#         pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_num_layers = overall_best_params['MACE']['PT']['num_layers']\n",
    "best_dropout = overall_best_params['MACE']['PT']['dropout']\n",
    "best_model_name = overall_best_params['MACE']['PT']['model_name']\n",
    "best_hidden_dim = overall_best_params['MACE']['PT']['hidden_dim']\n",
    "best_num_layers, best_dropout, best_hidden_dim, best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "val_r, val_loss, val_rmse = run_experiment(transformer_input_sequences_PT, None,\n",
    "            patient_indices_PT, transformer_outcomes_PT, transformer_input_lengths_PT, overall_best_params['MACE']['PT']['model_name'], \n",
    "    overall_best_params['MACE']['PT']['lr'], overall_best_params['MACE']['PT']['lr_decay'],\n",
    "       128, feature_types='EHR', model_path='', fine_tune=False, seed=3, \n",
    "    hidden_dim=overall_best_params['MACE']['PT']['hidden_dim'], num_layers=overall_best_params['MACE']['PT']['num_layers'], dropout=overall_best_params['stroke']['PT']['dropout'], hyperparam_tuning=False, vocab_size=len(merged_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_search = {\n",
    "    'batch_size': [16],\n",
    "    'lr': [1e-2,1e-3, 1e-4],\n",
    "    'dropout': [0.1, 0.3, 0.5],\n",
    "    'lr_decay': [1e-4, 1e-3],\n",
    "    'layers': [1,2, 4],\n",
    "    'hidden_dim': [128, 256]\n",
    "}\n",
    "\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assume that grid_search and all_params are already defined\n",
    "# all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]\n",
    "\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp1'] = {}\n",
    "\n",
    "    IDs = patient_indices_omics['person_id'].unique()\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in kf.split(IDs, transformer_outcomes_omics):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = IDs[train_index]\n",
    "        test_IDs = IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "\n",
    "\n",
    "        train_indices = patient_indices_omics[patient_indices_omics['person_id'].isin(train_IDs)]['index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices_omics[patient_indices_omics['person_id'].isin(test_IDs)]['index'].values\n",
    "        val_indices = patient_indices_omics[patient_indices_omics['person_id'].isin(val_IDs)]['index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = param_set['dropout']\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = param_set['layers']\n",
    "            hidden_dim = param_set['hidden_dim']\n",
    "            print(param_set)\n",
    "            val_r, val_loss, val_rmse = run_experiment(transformer_input_sequences_omics, None,\n",
    "                                                       patient_indices_omics, transformer_outcomes_omics,\n",
    "                                                       transformer_input_lengths_omics, 'EHR_omics_only',\n",
    "                                                       lr, lr_decay, bs, \n",
    "                                                       train_indices=train_indices, test_indices=test_indices,\n",
    "                                                       val_indices=val_indices, feature_types='EHR', model_path='',\n",
    "                                                       fine_tune=False, seed=42, hidden_dim=hidden_dim,\n",
    "                                                       num_layers=layers, dropout=dropout, hyperparam_tuning=True)\n",
    "\n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "\n",
    "        print('experiment 1')\n",
    "        print('outcome {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "\n",
    "print(num_layers, dropout, lr, lr_decay, hidden_dim, bs)\n",
    "\n",
    "overall_best_params['MACE']['exp1'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,'hidden_dim': int(hidden_dim), 'batch_size': int(bs)}\n",
    "with open(\"./best_hyperparams_transformer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp4'] = {}\n",
    "    results_dict = {}\n",
    "\n",
    "    # Iterate through the hyperparameter combinations\n",
    "    bs = 1000\n",
    "    lr = 0.1\n",
    "    dropout = best_dropout\n",
    "    lr_decay = 0.1\n",
    "    layers = best_num_layers\n",
    "    hidden_dim = best_hidden_dim\n",
    "    # print(param_set)\n",
    "    val_r, val_loss, val_rmse = run_experiment(transformer_input_sequences_omics, None,\n",
    "            patient_indices_omics, transformer_outcomes_omics, transformer_input_lengths_omics, 'EHR_omics_PT', \n",
    "            lr, lr_decay, bs, feature_types='EHR', model_path='./models/predictive_models/{}.pth'.format(best_model_name),\n",
    "                                               fine_tune=False, seed=42,\n",
    "                                               hidden_dim=hidden_dim,\n",
    "                                              num_layers=layers, dropout=dropout, hyperparam_tuning=False, vocab_size = len(merged_vocab))\n",
    "\n",
    "    results_dict[val_loss] = {'num_layers': layers,'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                            'hidden_dim': hidden_dim, 'batch_size': bs}\n",
    "\n",
    "    print('experiment 4')\n",
    "    print('outcome {}'.format(i))\n",
    "    overall_best_params[i]['exp4'] = results_dict[min(results_dict.keys())]\n",
    "    print(results_dict[min(results_dict.keys())])\n",
    "\n",
    "# Save the best hyperparameters to a pickle file\n",
    "with open(\"./best_hyperparams_transformer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_search = {\n",
    "    'batch_size': [16],\n",
    "    'lr': [1e-2,1e-3, 1e-4],\n",
    "    'dropout': [0.1, 0.3, 0.5],\n",
    "    'lr_decay': [1e-4, 1e-3],\n",
    "    'num_frozen':range(0, best_num_layers+1)}\n",
    "\n",
    "\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assume that grid_search and all_params are already defined\n",
    "# all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]\n",
    "\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "num_frozen_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp5'] = {}\n",
    "\n",
    "    IDs = patient_indices_omics['person_id'].unique()\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in kf.split(IDs, transformer_outcomes_omics):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = IDs[train_index]\n",
    "        test_IDs = IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "\n",
    "        train_indices = patient_indices_omics[patient_indices_omics['person_id'].isin(train_IDs)]['index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices_omics[patient_indices_omics['person_id'].isin(test_IDs)]['index'].values\n",
    "        val_indices = patient_indices_omics[patient_indices_omics['person_id'].isin(val_IDs)]['index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = best_dropout\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = best_num_layers\n",
    "            hidden_dim = best_hidden_dim\n",
    "            num_frozen = param_set['num_frozen']\n",
    "            print(param_set)\n",
    "            \n",
    "            val_r, val_loss, val_rmse = run_experiment(transformer_input_sequences_omics, None,\n",
    "            patient_indices_omics, transformer_outcomes_omics, transformer_input_lengths_omics, 'EHR_omics_PT_FT',\n",
    "                lr, lr_decay, bs, train_indices=train_indices, test_indices=test_indices,\n",
    "                val_indices=val_indices, feature_types='EHR',\n",
    "                model_path='./models/predictive_models/{}.pth'.format(best_model_name),\n",
    "                fine_tune=True, seed=42, hidden_dim=hidden_dim, num_layers=layers, dropout=dropout,\n",
    "                                                       hyperparam_tuning=True, vocab_size=len(merged_vocab), num_frozen=num_frozen)\n",
    "            \n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "            num_frozen_arr.append(num_frozen)\n",
    "\n",
    "        print('experiment 5')\n",
    "        print('outcome {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, num_frozen_arr,split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','num_frozen','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs','num_frozen']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs,num_frozen = hyperparam_df['val_loss'].idxmin()\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "print(num_layers, dropout, lr, lr_decay, hidden_dim, bs)\n",
    "\n",
    "overall_best_params['MACE']['exp5'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs),'num_frozen': int(num_frozen)}\n",
    "with open(\"./best_hyperparams_transformer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#experiment 1 = CB model EHR features\n",
    "#experiment 2 = CB model metab features\n",
    "#experiment 3 = CB model all features\n",
    "#experiment 4 = only pretrained model\n",
    "#experiment 5 = fine tune pretrained model \n",
    "#experiment 6 = add metabs, fine tune pretrained model\n",
    "\n",
    "num_iterations = 50\n",
    "results = {}\n",
    "for i in tqdm(outcome_list):\n",
    "    results[i] = {'exp1': [],'exp5':[]}\n",
    "    for j in tqdm(range(0, num_iterations)):\n",
    "        print('experiment 1')\n",
    "        val_auc = run_experiment(transformer_input_sequences_omics, None,\n",
    "            patient_indices_omics, transformer_outcomes_omics, transformer_input_lengths_omics, 'EHR_omics_only_{}'.format(j), \n",
    "                overall_best_params[i]['exp1']['lr'], overall_best_params[i]['exp1']['lr_decay'],\n",
    "                overall_best_params[i]['exp1']['batch_size'], feature_types='EHR', model_path='', fine_tune=False, seed=j,\n",
    "                                hidden_dim=overall_best_params[i]['exp1']['hidden_dim'],\n",
    "                                 num_layers=overall_best_params[i]['exp1']['num_layers'],\n",
    "                                 dropout=overall_best_params[i]['exp1']['dropout'], return_preds=True)\n",
    "        results[i]['exp1'].append(val_auc)\n",
    "        \n",
    "        # print('experiment 2')\n",
    "        # val_auc = run_experiment(transformer_input_sequences_omics, proteomics,\n",
    "        #     patient_indices, transformer_outcomes_omics, transformer_input_lengths_omics, 'proteomics_omics_only_{}'.format(j), \n",
    "        #         overall_best_params[i]['exp2']['lr'], overall_best_params[i]['exp2']['lr_decay'],\n",
    "        #         overall_best_params[i]['exp2']['batch_size'], overall_best_params[i]['exp2']['prot_hidden_dim'],\n",
    "        #                          feature_types='metab', model_path='', fine_tune=False, seed=j,\n",
    "        #                         hidden_dim=best_hidden_dim,num_layers=best_num_layers, dropout=best_dropout, return_preds=True)\n",
    "        # results[i]['exp2'].append(val_auc)\n",
    "        \n",
    "#         print('experiment 3')\n",
    "#         val_auc = run_experiment(transformer_input_sequences_omics, proteomics,\n",
    "#             patient_indices, transformer_outcomes_omics, transformer_input_lengths_omics, 'both_omics_only_{}'.format(j), \n",
    "#                 overall_best_params[i]['exp3']['lr'], overall_best_params[i]['exp3']['lr_decay'],\n",
    "#                 overall_best_params[i]['exp3']['batch_size'], overall_best_params[i]['exp3']['prot_hidden_dim'],\n",
    "#                                  feature_types='both', model_path='', fine_tune=False, seed=j,\n",
    "#                                 hidden_dim=overall_best_params[i]['exp3']['hidden_dim'],\n",
    "#                                  num_layers=overall_best_params[i]['exp3']['num_layers'],\n",
    "#                                  dropout=overall_best_params[i]['exp3']['dropout'],\n",
    "#                                  return_preds=True, return_interpretability=True, return_grads=True)\n",
    "#         results[i]['exp3'].append(val_auc)\n",
    "        \n",
    "#         print('experiment 4')\n",
    "#         val_auc = run_experiment(transformer_input_sequences_omics, proteomics,\n",
    "#             patient_indices, transformer_outcomes_omics, transformer_input_lengths_omics, 'EHR_PT_{}'.format(j), \n",
    "#                 overall_best_params[i]['exp4']['lr'], overall_best_params[i]['exp4']['lr_decay'],\n",
    "#                 overall_best_params[i]['exp4']['batch_size'], feature_types='EHR',\n",
    "#                                  model_path='./{}.pth'.format(best_model_name),\n",
    "#                                  fine_tune=False, seed=j,\n",
    "#                                 hidden_dim=best_hidden_dim,num_layers=best_num_layers,\n",
    "#                                  dropout=best_dropout, return_preds=True)\n",
    "#         results[i]['exp4'].append(val_auc)\n",
    "\n",
    "        print('experiment 5')\n",
    "        val_auc = run_experiment(transformer_input_sequences_omics, None,\n",
    "            patient_indices_omics, transformer_outcomes_omics, transformer_input_lengths_omics, 'EHR_PT_FT_{}'.format(j), \n",
    "                overall_best_params[i]['exp5']['lr'], overall_best_params[i]['exp5']['lr_decay'],\n",
    "                overall_best_params[i]['exp5']['batch_size'], feature_types='EHR',\n",
    "                                 model_path='./models/predictive_models/{}.pth'.format(best_model_name),\n",
    "                                 fine_tune=True, seed=j,\n",
    "                                hidden_dim=best_hidden_dim,num_layers=best_num_layers,\n",
    "                                 dropout=best_dropout, return_preds=True, vocab_size=len(merged_vocab),\n",
    "                                 num_frozen=overall_best_params[i]['exp5']['num_frozen'])\n",
    "        results[i]['exp5'].append(val_auc)\n",
    "\n",
    "#         print('experiment 6')\n",
    "#         val_auc = run_experiment(transformer_input_sequences_omics, proteomics,\n",
    "#             patient_indices, transformer_outcomes_omics, transformer_input_lengths_omics, 'both_PT_FT_{}'.format(j), \n",
    "#                 overall_best_params[i]['exp6']['lr'], overall_best_params[i]['exp6']['lr_decay'],\n",
    "#                 overall_best_params[i]['exp6']['batch_size'], overall_best_params[i]['exp6']['prot_hidden_dim'],\n",
    "#                                  feature_types='both',\n",
    "#                                 model_path='./{}.pth'.format(best_model_name),\n",
    "#                                  fine_tune=True, seed=j,\n",
    "#                                  hidden_dim=best_hidden_dim,num_layers=best_num_layers,\n",
    "#                                  dropout=best_dropout,\n",
    "#                                  return_preds=True, return_interpretability=True, return_grads=True)\n",
    "#         results[i]['exp6'].append(val_auc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, cohen_kappa_score\n",
    "\n",
    "for exp in ['exp1','exp5']:\n",
    "    true_outcomes = []\n",
    "    total_preds = []\n",
    "    indices = []\n",
    "    for index_num, i in enumerate(results['MACE'][exp]):\n",
    "        true_outcomes.extend(i[3])\n",
    "        total_preds.extend(i[4])\n",
    "        indices.extend(i[5])\n",
    "    \n",
    "    df = pd.DataFrame([true_outcomes, total_preds, indices]).T\n",
    "    df.columns = ['true_outcome', 'pred', 'index']\n",
    "    df = df.groupby('index').mean()\n",
    "    \n",
    "    print(exp)\n",
    "    print('AUC: {:.3f}'.format(roc_auc_score(df['true_outcome'], df['pred'])))\n",
    "    print('AUPRC: {:.3f}'.format(average_precision_score(df['true_outcome'], df['pred'])))\n",
    "    \n",
    "    epsilon = 1e-15\n",
    "    df['pred'] = np.clip(df['pred'], epsilon, 1 - epsilon)\n",
    "    \n",
    "    # Calculating the BCELoss\n",
    "    bce_loss = -np.mean(df['true_outcome'] * np.log(df['pred']) + (1 - df['true_outcome']) * np.log(1 - df['pred']))\n",
    "    print('BCELoss: {:.3f}'.format(bce_loss))\n",
    "    \n",
    "    # Convert predictions to binary for Cohen's Kappa\n",
    "    binary_preds = (df['pred'] > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate Cohen's Kappa\n",
    "    kappa = cohen_kappa_score(df['true_outcome'], binary_preds)\n",
    "    print('Cohen\\'s Kappa: {:.3f}'.format(kappa))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.pkl', 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
